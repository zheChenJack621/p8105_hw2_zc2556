P8105-HW2-zc2556
================
Zhe Chen
2020/9/28

## Problem 1

### Include relevent library

``` r
library(tidyverse)
```

    ## -- Attaching packages ------------------------------------------------------- tidyverse 1.3.0 --

    ## √ ggplot2 3.3.2     √ purrr   0.3.4
    ## √ tibble  3.0.3     √ dplyr   1.0.2
    ## √ tidyr   1.1.2     √ stringr 1.4.0
    ## √ readr   1.3.1     √ forcats 0.5.0

    ## -- Conflicts ---------------------------------------------------------- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(readxl)
```

### Import data

``` r
trashwheel =
  read_xlsx("./Trash-Wheel-Collection-Totals-8-6-19.xlsx", range = cell_cols("A:N")) %>% janitor::clean_names() %>%
drop_na(dumpster) %>%
mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
  )
```

### Data of Precipitation for 2018

``` r
precip_2018 = 
    read_excel(
        "./Trash-Wheel-Collection-Totals-8-6-19.xlsx",
        sheet = "2018 Precipitation",
        skip = 1
    ) %>% 
    janitor::clean_names() %>% 
    drop_na(month) %>% 
    mutate(year = 2018) %>% 
    relocate(year)
```

### Data of Precipitation for 2017

``` r
precip_2017 = 
    read_excel(
        "./Trash-Wheel-Collection-Totals-8-6-19.xlsx",
        sheet = "2017 Precipitation",
        skip = 1
    ) %>% 
    janitor::clean_names() %>% 
    drop_na(month) %>% 
    mutate(year = 2017) %>% 
    relocate(year)
```

### Combining 2018 and 2017 data

``` r
month_df = 
    tibble(
        month = 1:12,
        month_name = month.name
    )

precip = 
    bind_rows(precip_2018, precip_2017)

precip_df =
    left_join(precip, month_df, by = "month")
```

This dataset contains information from the Mr. Trashwheel trash
collector in Baltimore, Maryland. As trash enters the inner harbor, the
trashwheel collects that trash, and stores it in a dumpster. The dataset
contains information on year, month, and trash collected, include some
specific kinds of trash. There are a total of 344 rows in our final
dataset. Additional data sheets include month precipitation data. In
this dataset:

  - The median number of sports balls found in a dumpster in 2017 was 8
  - The total precipitation in 2018 was 70.33 inches.

## Problem 2

### Import and Clean the Data

``` r
# import and initial cleaning
NYC_Transit =
  read_csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% #clean the name
  select(line, name = station_name, route1:route11, station_latitude, station_longitude, entrance_type , entry, vending, ada) %>% #select meaningful variables
   mutate_at(vars(route1:route11), funs(as.character)) #convert to character for further cleaning
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_double(),
    ##   Route9 = col_double(),
    ##   Route10 = col_double(),
    ##   Route11 = col_double(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

``` r
# make the data more tidy
NYC_Transit_tidy = 
  NYC_Transit %>%
  # merging variables to a single variable
  pivot_longer( 
      route1:route11,
      names_to = "routes number",
      values_to = "routes name",
      names_prefix = "route"
    ) %>%
  drop_na("routes name") # abandon observations with missing values
NYC_Transit_tidy$entry = ifelse(NYC_Transit_tidy$entry == "YES", TRUE, FALSE) # convert entry to logical type
head(NYC_Transit_tidy) 
```

    ## # A tibble: 6 x 10
    ##   line  name  station_latitude station_longitu~ entrance_type entry vending
    ##   <chr> <chr>            <dbl>            <dbl> <chr>         <lgl> <chr>  
    ## 1 4 Av~ 25th~             40.7            -74.0 Stair         TRUE  YES    
    ## 2 4 Av~ 25th~             40.7            -74.0 Stair         TRUE  YES    
    ## 3 4 Av~ 36th~             40.7            -74.0 Stair         TRUE  YES    
    ## 4 4 Av~ 36th~             40.7            -74.0 Stair         TRUE  YES    
    ## 5 4 Av~ 36th~             40.7            -74.0 Stair         TRUE  YES    
    ## 6 4 Av~ 36th~             40.7            -74.0 Stair         TRUE  YES    
    ## # ... with 3 more variables: ada <lgl>, `routes number` <chr>, `routes
    ## #   name` <chr>

Above is the first 10 lines from the data set some cleaning. Our data
set, NYC\_Transit, contains the information about entrance, exit for
each subway in NYC. We cleaned the data set in several steps:

  - make the variable name to a uniform format;
  - keep variables that are useful(line, station name, routes, the
    latitude and the longitude of the station, entrance type, entry,
    vending, ADA), and rename some variables;
  - merge “route” variables into one variable “route” with variable
    “route served” which contains the transfer information and take
    off observations with missing values in “route served” variable;
  - convert entry’s data type to logical.

After cleaning with pivot\_longer, we have a more tidy data set with
dimension of 4270X10. This data set is pretty tidy now.

### Answer Questions and Reformat the Data

``` r
# find how many distinct stations
num_dis_stat = 
  NYC_Transit_tidy %>%
  distinct(line, name ) %>%
  nrow()
num_dis_stat
```

    ## [1] 465

``` r
# find how many stations are ADA compliant
num_ADA_comp = 
  NYC_Transit_tidy %>% 
  filter(ada == 1) %>%
  distinct(line, name) %>%
  nrow()
num_ADA_comp
```

    ## [1] 84

``` r
# entrances which allow entering but have no vending
num_nvend_enter_entrances = 
  NYC_Transit %>% filter((entry == 'YES')& (vending == 'NO')) %>%
  nrow()
num_no_vending = 
  NYC_Transit %>% filter(vending == 'NO') %>%
  nrow()
num_nvend_enter_entrances
```

    ## [1] 69

``` r
num_no_vending
```

    ## [1] 183

``` r
prop = num_nvend_enter_entrances/num_no_vending
prop
```

    ## [1] 0.3770492

We have 465 distinct stations; 84 stations with ADA compliance; lastly,
we have the proportion of 0.3770492.

``` r
# reformat the data by making route number and name distinctive

# number of distinct stations which serve A train
num_A =
  NYC_Transit_tidy %>%
  filter(`routes name` == 'A')%>%
  distinct(line, name) %>%
  nrow()
num_A
```

    ## [1] 60

``` r
# among them who are ADA compliant
num_A_ADA =
  NYC_Transit_tidy %>%
  filter(`routes name` == 'A')%>%
  filter(ada == TRUE) %>%
  distinct(line, name) %>%
  nrow()
num_A_ADA
```

    ## [1] 17

We have made the line number and line name variables pivot while
cleaning the data. We have 60 of stations which serve A train, and 17 of
them are ADA compliant.

## Problem 3

### Import, import and import.

``` r
# Import and clean pols_month.csv
pols_month =
  read_csv("./fivethirtyeight_datasets/pols-month.csv") %>% 
  janitor::clean_names() %>% #clean the name
  separate(mon, c("year", "month", "day")) %>%
  mutate(
    month = month.name[as.numeric(month)] #change month number to month names
    ) %>%
  mutate(
    president = prez_gop + prez_dem
  ) %>%
  select(
    -prez_dem, -prez_gop, -day
  )
```

    ## Parsed with column specification:
    ## cols(
    ##   mon = col_date(format = ""),
    ##   prez_gop = col_double(),
    ##   gov_gop = col_double(),
    ##   sen_gop = col_double(),
    ##   rep_gop = col_double(),
    ##   prez_dem = col_double(),
    ##   gov_dem = col_double(),
    ##   sen_dem = col_double(),
    ##   rep_dem = col_double()
    ## )

We import the data set “pols\_month.csv”. Interestingly, we have the
value “2” in “prez\_gop” during 1974 and it may because there was some
special historical event and we decide to keep it here. We have
variables containing the president, number of governors, senators and
representatives in both parties, year and month. It’s dimension is
822X9; year range is 1947, 2015; the key variables are year and month,
meaning that we can identify any observations with a combined year and
month.

``` r
#Import and clean snp.csv
snp =
  read_csv("./fivethirtyeight_datasets/snp.csv") %>% 
  janitor::clean_names() %>% #clean the name
  separate(date, c("month", "day", "year")) %>%
  mutate(
    month = month.abb[as.numeric(month)] #change month number to month names
  ) %>%
  select(-day) %>%
  relocate(year, month)
```

    ## Parsed with column specification:
    ## cols(
    ##   date = col_character(),
    ##   close = col_double()
    ## )

Similarly, we import and clean the “snp.csv”, making it consistent to
the pols\_month data. We have variables containing the date (year and
month) and closing value of the stock. It’s dimension is 787X3; year
range is 1950, 2015; the key variables are year and month, meaning that
we can identify any observations with a combined year and month.

``` r
#Import and clean unemployment.csv
unemployment =
  read_csv("./fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names() %>% #clean the name
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "unemployment_rate"
  ) #make it consistent with other two data sets
```

    ## Parsed with column specification:
    ## cols(
    ##   Year = col_double(),
    ##   Jan = col_double(),
    ##   Feb = col_double(),
    ##   Mar = col_double(),
    ##   Apr = col_double(),
    ##   May = col_double(),
    ##   Jun = col_double(),
    ##   Jul = col_double(),
    ##   Aug = col_double(),
    ##   Sep = col_double(),
    ##   Oct = col_double(),
    ##   Nov = col_double(),
    ##   Dec = col_double()
    ## )

We import the “unemployment.csv” and reformat it, consistent with the
other data sets. We have variables containing the date (year and month)
and unemployment rate at that year and month. It’s dimension is 816X3;
year range is 1950, 2015; the key variables are year and month, meaning
that we can identify any observations with a combined year and month.

### Join and Merge

Finally, we have 3 data sets with the same key variables (year and
month), and we now can join them and make one big data set.

``` r
snp$year = as.character(snp$year)
unemployment$year = as.character(unemployment$year)
final_ds =
  left_join(pols_month, snp)
```

    ## Joining, by = c("year", "month")

``` r
final_ds = 
  left_join(final_ds, unemployment)
```

    ## Joining, by = c("year", "month")

``` r
head(final_ds)
```

    ## # A tibble: 6 x 11
    ##   year  month gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president close
    ##   <chr> <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>     <dbl> <dbl>
    ## 1 1947  Janu~      23      51     253      23      45     198         1    NA
    ## 2 1947  Febr~      23      51     253      23      45     198         1    NA
    ## 3 1947  March      23      51     253      23      45     198         1    NA
    ## 4 1947  April      23      51     253      23      45     198         1    NA
    ## 5 1947  May        23      51     253      23      45     198         1    NA
    ## 6 1947  June       23      51     253      23      45     198         1    NA
    ## # ... with 1 more variable: unemployment_rate <dbl>

We merged three data sets by variable year and month and the final data
set contains all variables from three data sets. With this final data
set, we can check the relation between politics (which party is
dominant) and the economy of the society. However, since each data set
has a different year range, there are many missing values in this final
data set and we may need to clean those missing values for further
analysis.
