---
title: "P8105-HW2-zc2556"
author: "Zhe Chen"
date: "2020/9/28"
output: github_document
---

## Problem 1

### Include relevent library

```{r, warning=FALSE}
library(tidyverse)
library(readxl)
```


### Import data

```{r, warning = FALSE}
trashwheel =
  read_xlsx("./Trash-Wheel-Collection-Totals-8-6-19.xlsx", range = cell_cols("A:N")) %>% janitor::clean_names() %>%
drop_na(dumpster) %>%
mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
  )

```

### Data of Precipitation for 2018

```{r}
precip_2018 = 
	read_excel(
		"./Trash-Wheel-Collection-Totals-8-6-19.xlsx",
		sheet = "2018 Precipitation",
		skip = 1
	) %>% 
	janitor::clean_names() %>% 
	drop_na(month) %>% 
	mutate(year = 2018) %>% 
	relocate(year)
```

### Data of Precipitation for 2017

```{r}
precip_2017 = 
	read_excel(
		"./Trash-Wheel-Collection-Totals-8-6-19.xlsx",
		sheet = "2017 Precipitation",
		skip = 1
	) %>% 
	janitor::clean_names() %>% 
	drop_na(month) %>% 
	mutate(year = 2017) %>% 
	relocate(year)
```

### Combining 2018 and 2017 data

```{r}
month_df = 
	tibble(
		month = 1:12,
		month_name = month.name
	)

precip = 
	bind_rows(precip_2018, precip_2017)

precip_df =
	left_join(precip, month_df, by = "month")

```

This dataset contains information from the Mr. Trashwheel trash collector in Baltimore, Maryland. As trash enters the inner harbor, the trashwheel collects that trash, and stores it in a dumpster. The dataset contains information on year, month, and trash collected, include some specific kinds of trash. There are a total of `r nrow(trashwheel)` rows in our final dataset. Additional data sheets include month precipitation data. In this dataset:

* The median number of sports balls found in a dumpster in 2017 was `r trashwheel %>% filter(year == 2017) %>% pull(sports_balls) %>% median()`
* The total precipitation in 2018 was `r precip%>% filter(year == 2018) %>% pull(total) %>% sum()` inches.

## Problem 2

### Import and Clean the Data

```{r, warning= FALSE}
# import and initial cleaning
NYC_Transit =
  read_csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% #clean the name
  select(line, name = station_name, route1:route11, station_latitude, station_longitude, entrance_type , entry, vending, ada) %>% #select meaningful variables
  mutate_at(vars(route1:route11), funs(as.character)) #convert to character for further cleaning

# make the data more tidy
NYC_Transit_tidy = 
  NYC_Transit %>%
  # merging variables to a single variable
  pivot_longer( 
      route1:route11,
      names_to = "routes number",
      values_to = "routes name",
      names_prefix = "route"
    ) %>%
  drop_na("routes name") # abandon observations with missing values
NYC_Transit_tidy$entry = ifelse(NYC_Transit_tidy$entry == "YES", TRUE, FALSE) # convert entry to logical type
head(NYC_Transit_tidy) 
```

  Above is the first 10 lines from the data set some cleaning. Our data set, NYC_Transit, contains the information about entrance, exit for each subway in NYC. We cleaned the data set in several steps:
  
  * make the variable name to a uniform format;
  * keep variables that are useful(line, station name, routes, the latitude and the longitude of the station, entrance type, entry, vending, ADA), and rename some variables;
  * merge "route" variables into one variable "route" with variable "route served" which contains the transfer information and take off observations with missing values in "route served" variable; 
  * convert entry's data type to logical.
  
  After cleaning with pivot_longer, we have a more tidy data set with dimension of `r nrow(NYC_Transit_tidy)`X`r ncol(NYC_Transit_tidy)`. This data set is pretty tidy now.

### Answer Questions and Reformat the Data

```{r}
# find how many distinct stations
num_dis_stat = 
  NYC_Transit_tidy %>%
  distinct(line, name ) %>%
  nrow()
num_dis_stat

# find how many stations are ADA compliant
num_ADA_comp = 
  NYC_Transit_tidy %>% 
  filter(ada == 1) %>%
  distinct(line, name) %>%
  nrow()
num_ADA_comp

# entrances which allow entering but have no vending
num_nvend_enter_entrances = 
  NYC_Transit %>% filter((entry == 'YES')& (vending == 'NO')) %>%
  nrow()
num_no_vending = 
  NYC_Transit %>% filter(vending == 'NO') %>%
  nrow()
num_nvend_enter_entrances
num_no_vending
prop = num_nvend_enter_entrances/num_no_vending
prop
```
  We have `r num_dis_stat` distinct stations; `r num_ADA_comp` stations with ADA compliance; lastly, we have the proportion of `r prop`.
  
```{r}
# reformat the data by making route number and name distinctive

# number of distinct stations which serve A train
num_A =
  NYC_Transit_tidy %>%
  filter(`routes name` == 'A')%>%
  distinct(line, name) %>%
  nrow()
num_A

# among them who are ADA compliant
num_A_ADA =
  NYC_Transit_tidy %>%
  filter(`routes name` == 'A')%>%
  filter(ada == TRUE) %>%
  distinct(line, name) %>%
  nrow()
num_A_ADA
```
  We have made the line number and line name variables pivot while cleaning the data. We have `r num_A` of stations which serve A train, and `r num_A_ADA` of them are ADA compliant.

## Problem 3

### Import, import and import.

```{r}
# Import and clean pols_month.csv
pols_month =
  read_csv("./fivethirtyeight_datasets/pols-month.csv") %>% 
  janitor::clean_names() %>% #clean the name
  separate(mon, c("year", "month", "day")) %>%
  mutate(
    month = month.name[as.numeric(month)] #change month number to month names
    ) %>%
  mutate(
    president = ifelse(prez_gop == 0, "dem", "gop")
  ) %>%
  select(
    -prez_dem, -prez_gop, -day
  )
```

We import the data set "pols_month.csv". Interestingly, we have the value "2" in "prez_gop" during 1974 and it may because there was a special historical event and we decided to keep it here. We have variables containing the belonging party of the president, number of governors, senators and representatives in both parties, year and month. It's dimension is `r nrow(pols_month)`X`r ncol(pols_month)`; year range is `r range(pols_month$year)`; the key variables are year and month, meaning that we can identify any observations with a combined year and month.  

```{r}
#Import and clean snp.csv
snp =
  read_csv("./fivethirtyeight_datasets/snp.csv") %>% 
  janitor::clean_names() %>% #clean the name
  separate(date, c("month", "day", "year")) %>%
  mutate(
    month = month.abb[as.numeric(month)] #change month number to month names
  ) %>%
  select(-day) %>%
  relocate(year, month)
```
Similarly, we import and clean the "snp.csv", making it consistent to the pols_month data. We have variables containing the date (year and month) and closing value of the stock. It's dimension is `r nrow(snp)`X`r ncol(snp)`; year range is `r range(snp$year)`; the key variables are year and month, meaning that we can identify any observations with a combined year and month.

```{r}
#Import and clean unemployment.csv
unemployment =
  read_csv("./fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names() %>% #clean the name
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "unemployment_rate"
  ) #make it consistent with other two data sets
  
```
We import the "unemployment.csv" and reformat it, consistent with the other data sets. We have variables containing the date (year and month) and unemployment rate at that year and month. It's dimension is `r nrow(unemployment)`X`r ncol(unemployment)`; year range is `r range(snp$year)`; the key variables are year and month, meaning that we can identify any observations with a combined year and month.

### Join and Merge

Finally, we have 3 data sets with the same key variables (year and month), and we now can join them and make one big data set.

```{r}
snp$year = as.character(snp$year)
unemployment$year = as.character(unemployment$year)
final_ds =
  left_join(pols_month, snp)
final_ds = 
  left_join(final_ds, unemployment)
head(final_ds)
```
We merged three data sets by variable year and month and the final data set contains all variables from three data sets. With this final data set, we can check the relation between politics (which party is dominant) and the economy of the society. However, since each data set has a different year range, there are many missing values in this final data set and we may need to clean those missing values for further analysis. 







